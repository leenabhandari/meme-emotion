{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Emotion\n",
    "\n",
    "The main objective of this notebook is to predict emotions from tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project path to the PYTHONPATH\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(Path(os.path.join(os.path.abspath(''), '../')).resolve().as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer\n",
    "\n",
    "Load `.pickle` file with the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = Path('../datasets/sentiment_analysis/tokenizer.pickle').resolve()\n",
    "with tokenizer_path.open('rb') as file:\n",
    "    tokenizer = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "Load the trained emotion recognition model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, SpatialDropout1D, LSTM\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Bidirectional, Conv1D, Dense, concatenate\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = min(tokenizer.num_words, len(tokenizer.word_index) + 1)\n",
    "num_classes = 4\n",
    "embedding_dim = 500\n",
    "input_length = 100\n",
    "lstm_units = 128\n",
    "lstm_dropout = 0.1\n",
    "recurrent_dropout = 0.1\n",
    "spatial_dropout=0.2\n",
    "filters=64\n",
    "kernel_size=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(input_length,))\n",
    "output_layer = Embedding(\n",
    "  input_dim=input_dim,\n",
    "  output_dim=embedding_dim,\n",
    "  input_shape=(input_length,)\n",
    ")(input_layer)\n",
    "\n",
    "output_layer = SpatialDropout1D(spatial_dropout)(output_layer)\n",
    "\n",
    "output_layer = Bidirectional(\n",
    "LSTM(lstm_units, return_sequences=True,\n",
    "     dropout=lstm_dropout, recurrent_dropout=recurrent_dropout)\n",
    ")(output_layer)\n",
    "output_layer = Conv1D(filters, kernel_size=kernel_size, padding='valid',\n",
    "                    kernel_initializer='glorot_uniform')(output_layer)\n",
    "\n",
    "avg_pool = GlobalAveragePooling1D()(output_layer)\n",
    "max_pool = GlobalMaxPooling1D()(output_layer)\n",
    "output_layer = concatenate([avg_pool, max_pool])\n",
    "\n",
    "output_layer = Dense(num_classes, activation='softmax')(output_layer)\n",
    "\n",
    "model = Model(input_layer, output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights_path = Path('../models/emotion_recognition/model_weights.h5').resolve()\n",
    "model.load_weights(model_weights_path.as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Load the data that will have the labels predicted by the model\n",
    "\n",
    "**data_path**: Path to the `.csv` file that will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anger</td>\n",
       "      <td>1337090985638187009</td>\n",
       "      <td>2020-12-10 17:45:07</td>\n",
       "      <td>maahiwins</td>\n",
       "      <td>New day #angry maahi https://t.co/fUIR5zhu2R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anger</td>\n",
       "      <td>1337084058350784516</td>\n",
       "      <td>2020-12-10 17:17:35</td>\n",
       "      <td>SandPap39180050</td>\n",
       "      <td>@RoyalMail @RoyalMailHelp where is my parcel t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anger</td>\n",
       "      <td>1337055644294242306</td>\n",
       "      <td>2020-12-10 15:24:41</td>\n",
       "      <td>shady2405</td>\n",
       "      <td>It's been a really hard few weeks with loosing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anger</td>\n",
       "      <td>1337042537912135680</td>\n",
       "      <td>2020-12-10 14:32:36</td>\n",
       "      <td>AnnaLogue75</td>\n",
       "      <td>I'm fighting to get just the minimal dose to g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anger</td>\n",
       "      <td>1337017566959644673</td>\n",
       "      <td>2020-12-10 12:53:23</td>\n",
       "      <td>Ninnersgirl</td>\n",
       "      <td>@GetSpectrum A Spectrum tech comes to my house...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                   id                 date             user  \\\n",
       "0  anger  1337090985638187009  2020-12-10 17:45:07        maahiwins   \n",
       "1  anger  1337084058350784516  2020-12-10 17:17:35  SandPap39180050   \n",
       "2  anger  1337055644294242306  2020-12-10 15:24:41        shady2405   \n",
       "3  anger  1337042537912135680  2020-12-10 14:32:36      AnnaLogue75   \n",
       "4  anger  1337017566959644673  2020-12-10 12:53:23      Ninnersgirl   \n",
       "\n",
       "                                                text  \n",
       "0       New day #angry maahi https://t.co/fUIR5zhu2R  \n",
       "1  @RoyalMail @RoyalMailHelp where is my parcel t...  \n",
       "2  It's been a really hard few weeks with loosing...  \n",
       "3  I'm fighting to get just the minimal dose to g...  \n",
       "4  @GetSpectrum A Spectrum tech comes to my house...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = Path('../datasets/sentiment_analysis/dataset.csv').resolve()\n",
    "#../datasets/predict/1151893341782585349-1151863653320159233_kyoto_animation.csv\n",
    "data = pd.read_csv(data_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Encoder\n",
    "\n",
    "Load `.pickle` file with the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_path = Path('../models/emotion_recognition/encoder.pickle').resolve()\n",
    "with encoder_path.open('rb') as file:\n",
    "    encoder = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data\n",
    "\n",
    "Preprocess the data that will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\leena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nlp import preprocess\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to clean up: 8.24 sec\n"
     ]
    }
   ],
   "source": [
    "cleaned_data = preprocess(data.text)\n",
    "sequences = [text.split() for text in cleaned_data]\n",
    "list_tokenized = tokenizer.texts_to_sequences(sequences)\n",
    "x_data = pad_sequences(list_tokenized, maxlen=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Predict the labels and generate a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger: 0.015743947\n",
      "fear: 0.08909034\n",
      "joy: 0.43358883\n",
      "sadness: 0.46157965\n"
     ]
    }
   ],
   "source": [
    "for index, value in enumerate(np.sum(y_pred, axis=0) / len(y_pred)):\n",
    "    print(encoder.classes_[index] + \": \" + str(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger: 0.00017329520838748808\n",
      "fear: 0.10865609565895502\n",
      "joy: 0.43072524044710164\n",
      "sadness: 0.46044536868555586\n"
     ]
    }
   ],
   "source": [
    "y_pred_argmax = y_pred.argmax(axis=1)\n",
    "data_len = len(y_pred_argmax)\n",
    "for index, value in enumerate(np.unique(y_pred_argmax)):\n",
    "    print(encoder.classes_[index] + \": \" + str(len(y_pred_argmax[y_pred_argmax == value]) / data_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 3], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[5:10].argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Listen to \"How Are You Coping Through the Pandemic? Several Strategies That can Help you!\" by Grandpa Jim. âš“ https://t.co/BjiZKfMQ7V #richmond #harrisburg #raleigh #mke #annapolis #dover #norfolk #mad #sad #angry #happy #fun #lol #lmao #jacksonville #jax #sd #nd'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.text.iloc[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
