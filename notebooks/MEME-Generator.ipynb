{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 'entertainment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"http://newsapi.org/v2/top-headlines?country=in&category=\"+category+\"&apiKey=469e3b5dc7b44bf6ae90867fe0e8fd5c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = data['articles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = []\n",
    "for article in articles:\n",
    "    title_parts = article['title'].split(\" - \")\n",
    "    text = title_parts[0] + article['description']\n",
    "    news.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(Path(os.path.join(os.path.abspath(''), '../')).resolve().as_posix())\n",
    "import pickle\n",
    "\n",
    "tokenizer_path = Path('../datasets/sentiment_analysis/tokenizer.pickle').resolve()\n",
    "with tokenizer_path.open('rb') as file:\n",
    "    tokenizer = pickle.load(file)\n",
    "\n",
    "from tensorflow.keras.layers import Input, Embedding, SpatialDropout1D, LSTM\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Bidirectional, Conv1D, Dense, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "input_dim = min(tokenizer.num_words, len(tokenizer.word_index) + 1)\n",
    "num_classes = 4\n",
    "embedding_dim = 500\n",
    "input_length = 100\n",
    "lstm_units = 128\n",
    "lstm_dropout = 0.1\n",
    "recurrent_dropout = 0.1\n",
    "spatial_dropout=0.2\n",
    "filters=64\n",
    "kernel_size=3\n",
    "\n",
    "input_layer = Input(shape=(input_length,))\n",
    "output_layer = Embedding(\n",
    "  input_dim=input_dim,\n",
    "  output_dim=embedding_dim,\n",
    "  input_shape=(input_length,)\n",
    ")(input_layer)\n",
    "\n",
    "output_layer = SpatialDropout1D(spatial_dropout)(output_layer)\n",
    "\n",
    "output_layer = Bidirectional(\n",
    "LSTM(lstm_units, return_sequences=True,\n",
    "     dropout=lstm_dropout, recurrent_dropout=recurrent_dropout)\n",
    ")(output_layer)\n",
    "output_layer = Conv1D(filters, kernel_size=kernel_size, padding='valid',\n",
    "                    kernel_initializer='glorot_uniform')(output_layer)\n",
    "\n",
    "avg_pool = GlobalAveragePooling1D()(output_layer)\n",
    "max_pool = GlobalMaxPooling1D()(output_layer)\n",
    "output_layer = concatenate([avg_pool, max_pool])\n",
    "\n",
    "output_layer = Dense(num_classes, activation='softmax')(output_layer)\n",
    "\n",
    "model = Model(input_layer, output_layer)\n",
    "\n",
    "model_weights_path = Path('../models/emotion_recognition/model_weights.h5').resolve()\n",
    "model.load_weights(model_weights_path.as_posix())\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "encoder_path = Path('../models/emotion_recognition/encoder.pickle').resolve()\n",
    "with encoder_path.open('rb') as file:\n",
    "    encoder = pickle.load(file)\n",
    "    \n",
    "from nlp import preprocess\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = \"There was an idea, to bring together a group of remarkable people, to see if we could become something more. So when they needed us we could fight the battles that they never could. In time, you'll know what it's like to lose. To feel so desperately that you're right, that to fail all the same. Dread it, Run from it. Destiny still arrives. Evacuate the city. Engage all defences and get this man a shield. Fun isn't something one considers the balance in the universe. But this does put a smile on my face. Who the hell are you guys?\"\n",
    "dialogs = subs.split(\". \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger: 0.13067813\n",
      "fear: 0.7585651\n",
      "joy: 0.021767307\n",
      "sadness: 0.08898952\n"
     ]
    }
   ],
   "source": [
    "selected = dialogs[9]\n",
    "sequences = [text.split() for text in selected]\n",
    "list_tokenized = tokenizer.texts_to_sequences(sequences)\n",
    "x_data = pad_sequences(list_tokenized, maxlen=100)\n",
    "\n",
    "y_pred = model.predict(x_data)\n",
    "emo = dict()\n",
    "for index, value in enumerate(np.sum(y_pred, axis=0) / len(y_pred)):\n",
    "    print(encoder.classes_[index] + \": \" + str(value))\n",
    "    emo[encoder.classes_[index]]=value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_diff = 5\n",
    "ans = 0\n",
    "for i in range(len(news)):\n",
    "    cleaned_data = news[i]\n",
    "    sequences = [text.split() for text in cleaned_data]\n",
    "    list_tokenized = tokenizer.texts_to_sequences(sequences)\n",
    "    x_data = pad_sequences(list_tokenized, maxlen=100)\n",
    "\n",
    "    y_pred = model.predict(x_data)\n",
    "    \n",
    "    cur_diff = 0\n",
    "    for index, value in enumerate(np.sum(y_pred, axis=0) / len(y_pred)):\n",
    "        cur_diff += abs(emo[encoder.classes_[index]] - value)\n",
    "\n",
    "    if cur_diff < min_diff:\n",
    "        ans = i\n",
    "        min_diff = cur_diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horoscope today: December 27, 2020Vogue Daily Horoscope, 27th December 2020: Today's prediction for all zodiac signs: Aries, Taurus, Gemini, Cancer, Leo, Virgo. Libra, Scorpio, Sagittarius, Capricorn, Aquarius, Pisces. Check out our daily horoscope for free : But this does put a smile on my face\n"
     ]
    }
   ],
   "source": [
    "print(news[ans] + \" : \" + selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger: 0.13265857\n",
      "fear: 0.7518926\n",
      "joy: 0.021220021\n",
      "sadness: 0.09422911\n"
     ]
    }
   ],
   "source": [
    "cleaned_data = news[1]\n",
    "sequences = [text.split() for text in cleaned_data]\n",
    "list_tokenized = tokenizer.texts_to_sequences(sequences)\n",
    "x_data = pad_sequences(list_tokenized, maxlen=100)\n",
    "\n",
    "y_pred = model.predict(x_data)\n",
    "for index, value in enumerate(np.sum(y_pred, axis=0) / len(y_pred)):\n",
    "    print(encoder.classes_[index] + \": \" + str(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "\n",
    "def generateResult(videoClip,text, dialog):\n",
    "    cap = cv2.VideoCapture(videoClip) \n",
    "\n",
    "    frame_width = int(cap.get(3)) \n",
    "    frame_height = int(cap.get(4)) \n",
    "\n",
    "    size = (frame_width, frame_height) \n",
    "   \n",
    "    # Below VideoWriter object will create \n",
    "    # a frame of above defined The output  \n",
    "    # is stored in 'filename.avi' file. \n",
    "    out = cv2.VideoWriter('Meme_'+videoClip[:8] + text[:8]+ '.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 10, (frame_width,frame_height))\n",
    "\n",
    "\n",
    "    while(True): \n",
    "\n",
    "        # Capture frames in the video \n",
    "        ret, frame = cap.read() \n",
    "        if ret == False:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "        # describe the type of font \n",
    "        # to be used. \n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX \n",
    "\n",
    "        # Use putText() method for \n",
    "        # inserting text on video \n",
    "\n",
    "        cv2.putText(frame,  \n",
    "                    text,  \n",
    "                    (50, 50),  \n",
    "                    font, 0.8,  \n",
    "                    (100, 250, 230),  \n",
    "                    2,  \n",
    "                    cv2.LINE_4) \n",
    "        \n",
    "        cv2.putText(frame,  \n",
    "                    dialog,  \n",
    "                    (50, 150),  \n",
    "                    font, 0.7,  \n",
    "                    (0, 255, 255),  \n",
    "                    2,  \n",
    "                    cv2.LINE_4) \n",
    "\n",
    "        # Display the resulting frame \n",
    "        cv2.imshow('resultOutput', frame) \n",
    "        out.write(frame)\n",
    "\n",
    "        # creating 'q' as the quit  \n",
    "        # button for the video \n",
    "        if cv2.waitKey(22) & 0xFF == ord('q'): \n",
    "            break\n",
    "\n",
    "    # release the cap object \n",
    "    cap.release() \n",
    "    # close all windows \n",
    "    cv2.destroyAllWindows() \n",
    "    out.release()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_newlines(string, every=64):\n",
    "    return '\\n'.join(string[i:i+every] for i in range(0, len(string), every))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMeme(dialogs,news):\n",
    "    movie_line = 0\n",
    "    pairs = dict()\n",
    "    global_diff = 6\n",
    "    for j in range(len(dialogs)):\n",
    "        selected = dialogs[j]\n",
    "        sequences = [text.split() for text in selected]\n",
    "        list_tokenized = tokenizer.texts_to_sequences(sequences)\n",
    "        x_data = pad_sequences(list_tokenized, maxlen=100)\n",
    "\n",
    "        y_pred = model.predict(x_data)\n",
    "        emo = dict()\n",
    "        for index, value in enumerate(np.sum(y_pred, axis=0) / len(y_pred)):\n",
    "            print(encoder.classes_[index] + \": \" + str(value))\n",
    "            emo[encoder.classes_[index]]=value\n",
    "\n",
    "        min_diff = 3\n",
    "        news_line = 0\n",
    "        for i in range(len(news)):\n",
    "            cleaned_data = news[i]\n",
    "            sequences = [text.split() for text in cleaned_data]\n",
    "            list_tokenized = tokenizer.texts_to_sequences(sequences)\n",
    "            x_data = pad_sequences(list_tokenized, maxlen=100)\n",
    "\n",
    "            y_pred = model.predict(x_data)\n",
    "\n",
    "            cur_diff = 0\n",
    "            for index, value in enumerate(np.sum(y_pred, axis=0) / len(y_pred)):\n",
    "                cur_diff += abs(emo[encoder.classes_[index]] - value)\n",
    "\n",
    "            if cur_diff < min_diff:\n",
    "                news_line = i\n",
    "                min_diff = cur_diff\n",
    "\n",
    "        if min_diff < global_diff:\n",
    "            movie_line = j\n",
    "            global_diff = min_diff\n",
    "            \n",
    "        disp = articles[news_line]['title'] + \" : \" + dialogs[j]\n",
    "        print(articles[news_line]['title'] + \" : \" + dialogs[j] )\n",
    "        text = insert_newlines(disp,40)\n",
    "        generateResult(dialogs[j][:8] + '.mp4', articles[news_line]['title'], dialogs[j] )\n",
    "        pairs[j] = news_line\n",
    "    \n",
    "    #print(\"Final Output --> \"+ news[pairs[movie_line]] + \" : \" + dialogs[movie_line])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger: 0.13230877\n",
      "fear: 0.75031745\n",
      "joy: 0.020764206\n",
      "sadness: 0.0966099\n",
      "Kangana Ranaut’s first director Anurag Basu claims he doesn’t understand her ‘public persona’ - Times of India : There was an idea, to bring together a group of remarkable people, to see if we could become something more\n",
      "anger: 0.13240752\n",
      "fear: 0.75724274\n",
      "joy: 0.022118635\n",
      "sadness: 0.08823136\n",
      "'Get well soon, Soorya': Mammootty wishes ‘Thalapathi’ Rajini a speedy recovery - The News Minute : So when they needed us we could fight the battles that they never could\n",
      "anger: 0.13177775\n",
      "fear: 0.7610933\n",
      "joy: 0.022968303\n",
      "sadness: 0.0841608\n",
      "Celebrate Christmas With Alia Bhatt: Have A Look How It's Done - IWMBuzz : In time, you'll know what it's like to lose\n",
      "anger: 0.13157234\n",
      "fear: 0.7553917\n",
      "joy: 0.021886934\n",
      "sadness: 0.09114935\n",
      "See Pic: ‘Cheers to the last weekend of the year’, Priyanka Chopra bids adieu to 2020 - Times of India : To feel so desperately that you're right, that to fail all the same\n",
      "anger: 0.13294794\n",
      "fear: 0.75034225\n",
      "joy: 0.020553686\n",
      "sadness: 0.096156076\n",
      "Kangana Ranaut’s first director Anurag Basu claims he doesn’t understand her ‘public persona’ - Times of India : Dread it, Run from it\n",
      "anger: 0.13421217\n",
      "fear: 0.7557998\n",
      "joy: 0.021604886\n",
      "sadness: 0.08838318\n",
      "Harsh Varrdhan Kapoor clarifies diss against Vikramaditya Motwane, Bhavesh Joshi in AK vs AK: ‘I’m not... - Hindustan Times : Destiny still arrives\n",
      "anger: 0.1333073\n",
      "fear: 0.75831497\n",
      "joy: 0.022379177\n",
      "sadness: 0.08599846\n",
      "'Get well soon, Soorya': Mammootty wishes ‘Thalapathi’ Rajini a speedy recovery - The News Minute : Evacuate the city\n",
      "anger: 0.1350403\n",
      "fear: 0.7442698\n",
      "joy: 0.020878615\n",
      "sadness: 0.09981136\n",
      "Watch Video: Kangana Ranaut urges ‘don't immerse my ashes in Ganga’ in latest poem - Times of India : Engage all defences and get this man a shield\n",
      "anger: 0.1340147\n",
      "fear: 0.75192827\n",
      "joy: 0.021538613\n",
      "sadness: 0.092518605\n",
      "On Navya Nanda's Post, Rumoured Boyfriend Meezaan Jaaferi Left This Comment - NDTV : Fun isn't something one considers the balance in the universe\n",
      "anger: 0.13067813\n",
      "fear: 0.7585651\n",
      "joy: 0.021767307\n",
      "sadness: 0.08898952\n",
      "'Get well soon, Soorya': Mammootty wishes ‘Thalapathi’ Rajini a speedy recovery - The News Minute : But this does put a smile on my face\n",
      "anger: 0.13135451\n",
      "fear: 0.75693786\n",
      "joy: 0.022167336\n",
      "sadness: 0.08954038\n",
      "'Get well soon, Soorya': Mammootty wishes ‘Thalapathi’ Rajini a speedy recovery - The News Minute : Who the hell are you guys?\n"
     ]
    }
   ],
   "source": [
    "getMeme(dialogs,news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
